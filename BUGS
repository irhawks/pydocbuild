# 问题报告


pydoit的数据库只在任务执行完毕的时候才写入.doit.db.*文件中。因此如果任务数比较多，比如说有上千个网页任务，那么很容易造成内存不足的。在这种情况下，一般选择几百个任务完成之后再进行下一轮的操作。这个时候可以使用保存的文件来列出：比如每两百个文件就执行一次任务，然后再重启一次任务。

至于使用sqlite3、dbm或者json后端作为数据库，其实影响并不大。


```
IFS=''
for i in `./maker.py list | grep "SAVE " | sed 's/SAVE \(.*[^ ]\)\s\+$/\1/' | awk '{if (NR%200==0){print $0} else {printf"%s ",$0}}'`;
do
    echo $i
done
```

可以使用如下的Shell脚本。表示首先从maker.py得到任务的列表，然后把需要保存的文件名找出来(sed)，这个时候每一行是一个任务。此时我们换一种方式，选择awk，让每两百个文件就合并一次。这个时候就可以得到分批次处理的结果了。
